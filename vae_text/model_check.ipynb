{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "automated-division",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "# tf.config.experimental_run_functions_eagerly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "arranged-idaho",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 500\n",
    "lr = 0.0003\n",
    "droprate = 0.2\n",
    "batch_size = 128\n",
    "kl_weight = 1\n",
    "\n",
    "intr_dim = 128\n",
    "latent_dim = 64\n",
    "\n",
    "max_length = 50 #time steps\n",
    "emb_dim = 300\n",
    "vocab_size = 24888"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "configured-softball",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /users/apokkunu/trial/akarsh/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:1666: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "Model: \"VAE\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 50)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 50, 300)      7466400     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "custom_lstm (custom_lstm)       (None, 256)          439296      embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "z_mean (Dense)                  (None, 64)           16448       custom_lstm[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "z_log_var (Dense)               (None, 64)           16448       custom_lstm[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "sampling (Sampling)             (None, 64)           0           z_mean[0][0]                     \n",
      "                                                                 z_log_var[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "custom_decoder (custom_decoder) (None, 50, 24888)    6889272     sampling[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "elbo__layer (ELBO_Layer)        (None, 50)           0           custom_decoder[0][0]             \n",
      "==================================================================================================\n",
      "Total params: 14,827,864\n",
      "Trainable params: 7,361,464\n",
      "Non-trainable params: 7,466,400\n",
      "__________________________________________________________________________________________________\n",
      "layer 0: <tensorflow.python.keras.engine.input_layer.InputLayer object at 0x2b57d45d2a10>\n",
      "has input mask: None\n",
      "has output mask: None\n",
      "layer 1: <tensorflow.python.keras.layers.embeddings.Embedding object at 0x2b57d4649d90>\n",
      "has input mask: None\n",
      "has output mask: Tensor(\"embedding/NotEqual:0\", shape=(None, 50), dtype=bool)\n",
      "layer 2: <__main__.custom_lstm object at 0x2b57d4649f50>\n",
      "has input mask: Tensor(\"embedding/NotEqual:0\", shape=(None, 50), dtype=bool)\n",
      "has output mask: Tensor(\"embedding/NotEqual:0\", shape=(None, 50), dtype=bool)\n",
      "layer 3: <tensorflow.python.keras.layers.core.Dense object at 0x2b57d472f390>\n",
      "has input mask: Tensor(\"embedding/NotEqual:0\", shape=(None, 50), dtype=bool)\n",
      "has output mask: Tensor(\"embedding/NotEqual:0\", shape=(None, 50), dtype=bool)\n",
      "layer 4: <tensorflow.python.keras.layers.core.Dense object at 0x2b57d46e5f50>\n",
      "has input mask: Tensor(\"embedding/NotEqual:0\", shape=(None, 50), dtype=bool)\n",
      "has output mask: Tensor(\"embedding/NotEqual:0\", shape=(None, 50), dtype=bool)\n",
      "layer 5: <__main__.Sampling object at 0x2b58057b9f50>\n",
      "has input mask: [<tf.Tensor 'embedding/NotEqual:0' shape=(None, 50) dtype=bool>, <tf.Tensor 'embedding/NotEqual:0' shape=(None, 50) dtype=bool>]\n",
      "has output mask: Tensor(\"embedding/NotEqual:0\", shape=(None, 50), dtype=bool)\n",
      "layer 6: <__main__.custom_decoder object at 0x2b58057e46d0>\n",
      "has input mask: Tensor(\"embedding/NotEqual:0\", shape=(None, 50), dtype=bool)\n",
      "has output mask: Tensor(\"embedding/NotEqual:0\", shape=(None, 50), dtype=bool)\n",
      "layer 7: <__main__.ELBO_Layer object at 0x2b580581d710>\n",
      "has input mask: Tensor(\"embedding/NotEqual:0\", shape=(None, 50), dtype=bool)\n",
      "has output mask: Tensor(\"embedding/NotEqual:0\", shape=(None, 50), dtype=bool)\n"
     ]
    }
   ],
   "source": [
    "####################################################################################\n",
    "\n",
    "class Sampling(tf.keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(Sampling, self).__init__()\n",
    "        self.supports_masking = True\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        epsilon = tf.random.normal([batch, latent_dim])\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "class custom_lstm(tf.keras.layers.Layer):\n",
    "    def __init__(self, intr_dim, droprate,  **kwargs):\n",
    "        self.bi_lstm = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(intr_dim, recurrent_dropout=droprate, \n",
    "                                                                          return_sequences=False), merge_mode='concat')\n",
    "        self.drop_layer = tf.keras.layers.Dropout(droprate)\n",
    "        super(custom_lstm, self).__init__(**kwargs)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        h = self.bi_lstm(inputs)\n",
    "        h = self.drop_layer(h)\n",
    "        return h\n",
    "    \n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        return mask\n",
    "    \n",
    "x = tf.keras.layers.Input(shape=(max_length,))\n",
    "embed_layer = tf.keras.layers.Embedding(vocab_size, emb_dim, input_length=max_length, trainable=False, mask_zero=True)\n",
    "encoder_layer = custom_lstm(intr_dim, droprate)\n",
    "\n",
    "h = embed_layer(x)\n",
    "h = encoder_layer(h)\n",
    "z_mean = tf.keras.layers.Dense(latent_dim, name='z_mean')(h)\n",
    "z_log_var = tf.keras.layers.Dense(latent_dim, name='z_log_var')(h)\n",
    "z = Sampling()([z_mean, z_log_var])\n",
    "\n",
    "####################################################################################\n",
    "\n",
    "class custom_decoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, vocab_size, intr_dim, max_length, droprate, **kwargs):\n",
    "        self.rpv = tf.keras.layers.RepeatVector(max_length)\n",
    "        self.lstm_layer_1 = tf.keras.layers.LSTM(intr_dim, return_sequences=True, recurrent_dropout=droprate)\n",
    "        self.droplayer_2 = tf.keras.layers.Dropout(droprate)\n",
    "        self.lstm_layer_2 = tf.keras.layers.LSTM(intr_dim*2, return_sequences=True, recurrent_dropout=droprate)\n",
    "        self.droplayer_3 = tf.keras.layers.Dropout(droprate)\n",
    "        self.decoded_logits = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(vocab_size, activation='linear'))\n",
    "        super(custom_decoder, self).__init__(**kwargs)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        h = self.rpv(inputs)\n",
    "        h = self.lstm_layer_1(h)\n",
    "        h = self.droplayer_2(h)\n",
    "        h = self.lstm_layer_2(h)\n",
    "        h = self.droplayer_3(h)\n",
    "        decoded = self.decoded_logits(h)\n",
    "        return decoded\n",
    "    \n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        return mask\n",
    "    \n",
    "decoder_layer = custom_decoder(vocab_size, intr_dim, max_length, droprate)\n",
    "decoded_logits = decoder_layer(z)\n",
    "\n",
    "####################################################################################\n",
    "\n",
    "class ELBO_Layer(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(ELBO_Layer, self).__init__(**kwargs)\n",
    "    \n",
    "    def call(self, inputs, mask=None):\n",
    "        broadcast_float_mask = tf.cast(mask, \"float32\")\n",
    "        labels = tf.cast(x, tf.int32)\n",
    "        reconstruction_loss = tf.reduce_sum(tfa.seq2seq.sequence_loss(inputs, labels, \n",
    "                                                                      weights=broadcast_float_mask,\n",
    "                                                                      average_across_timesteps=False,\n",
    "                                                                      average_across_batch=False), axis=1)\n",
    "        \n",
    "        kl_loss = - 0.5 * tf.reduce_sum(1 + z_log_var - tf.math.square(z_mean) - tf.math.exp(z_log_var), axis=1)\n",
    "        total_loss = tf.reduce_mean(reconstruction_loss + kl_weight * kl_loss)\n",
    "        self.add_loss(total_loss, inputs=[x, inputs])\n",
    "        return tf.ones_like(x)\n",
    "    \n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        return mask\n",
    "        \n",
    "elbo_layer = ELBO_Layer()\n",
    "fake_decoded_prob = elbo_layer(decoded_logits)\n",
    "\n",
    "####################################################################################\n",
    "\n",
    "def zero_loss(y_true, y_pred):\n",
    "    return tf.zeros_like(y_pred)\n",
    "\n",
    "def kl_loss(x, fake_decoded_prob):\n",
    "    kl_loss = - 0.5 * tf.reduce_sum(1 + z_log_var - tf.math.square(z_mean) - tf.math.exp(z_log_var), axis=1)\n",
    "    kl_loss = kl_weight * kl_loss\n",
    "    return tf.reduce_mean(kl_loss)\n",
    "\n",
    "vae = tf.keras.models.Model(x, fake_decoded_prob, name='VAE')\n",
    "opt = tf.keras.optimizers.Adam(lr=lr)\n",
    "vae.compile(optimizer=opt, loss=[zero_loss], metrics=[kl_loss])\n",
    "vae.summary()\n",
    "\n",
    "for i, l in enumerate(vae.layers):\n",
    "    print(f'layer {i}: {l}', flush=True)\n",
    "    print(f'has input mask: {l.input_mask}', flush=True)\n",
    "    print(f'has output mask: {l.output_mask}', flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "pressed-genius",
   "metadata": {},
   "outputs": [],
   "source": [
    "# logits = tf.random.normal([batch_size, max_length, vocab_size], dtype=tf.float32)\n",
    "# targets = tf.random.uniform([batch_size, max_length], minval=0, maxval=vocab_size, dtype=tf.int32)\n",
    "# proj_w = tf.random.normal([vocab_size, vocab_size], dtype=tf.float32)\n",
    "# proj_b = tf.zeros(vocab_size, dtype=tf.float32)\n",
    "\n",
    "# print(logits.shape, targets.shape, proj_w.shape, proj_b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "social-knight",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 0: <tensorflow.python.keras.engine.input_layer.InputLayer object at 0x2b57d45d2a10>\n",
      "has input mask: None\n",
      "has output mask: None\n",
      "layer 1: <tensorflow.python.keras.layers.embeddings.Embedding object at 0x2b57d4649d90>\n",
      "has input mask: None\n",
      "has output mask: Tensor(\"embedding/NotEqual:0\", shape=(None, 50), dtype=bool)\n",
      "layer 2: <__main__.custom_lstm object at 0x2b57d4649f50>\n",
      "has input mask: Tensor(\"embedding/NotEqual:0\", shape=(None, 50), dtype=bool)\n",
      "has output mask: Tensor(\"embedding/NotEqual:0\", shape=(None, 50), dtype=bool)\n",
      "layer 3: <tensorflow.python.keras.layers.core.Dense object at 0x2b57d472f390>\n",
      "has input mask: Tensor(\"embedding/NotEqual:0\", shape=(None, 50), dtype=bool)\n",
      "has output mask: Tensor(\"embedding/NotEqual:0\", shape=(None, 50), dtype=bool)\n",
      "layer 4: <tensorflow.python.keras.layers.core.Dense object at 0x2b57d46e5f50>\n",
      "has input mask: Tensor(\"embedding/NotEqual:0\", shape=(None, 50), dtype=bool)\n",
      "has output mask: Tensor(\"embedding/NotEqual:0\", shape=(None, 50), dtype=bool)\n",
      "layer 5: <__main__.Sampling object at 0x2b58057b9f50>\n",
      "has input mask: [<tf.Tensor 'embedding/NotEqual:0' shape=(None, 50) dtype=bool>, <tf.Tensor 'embedding/NotEqual:0' shape=(None, 50) dtype=bool>]\n",
      "has output mask: Tensor(\"embedding/NotEqual:0\", shape=(None, 50), dtype=bool)\n"
     ]
    }
   ],
   "source": [
    "# build a model to project sentences on the latent space\n",
    "encoder = tf.keras.models.Model(x, z)\n",
    "\n",
    "for i, l in enumerate(encoder.layers):\n",
    "    print(f'layer {i}: {l}', flush=True)\n",
    "    print(f'has input mask: {l.input_mask}', flush=True)\n",
    "    print(f'has output mask: {l.output_mask}', flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "common-compilation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 0: <tensorflow.python.keras.engine.input_layer.InputLayer object at 0x2b5805f57b90>\n",
      "has input mask: None\n",
      "has output mask: None\n",
      "layer 1: <__main__.custom_decoder object at 0x2b58057e46d0>\n",
      "has input mask: Tensor(\"embedding/NotEqual:0\", shape=(None, 50), dtype=bool)\n",
      "has output mask: Tensor(\"embedding/NotEqual:0\", shape=(None, 50), dtype=bool)\n"
     ]
    }
   ],
   "source": [
    "# build a generator that can sample sentences from the learned distribution\n",
    "ins = tf.keras.layers.Input(shape=(latent_dim,))\n",
    "x_logits = decoder_layer(ins)\n",
    "generator = tf.keras.models.Model(ins, x_logits)\n",
    "\n",
    "for i, l in enumerate(generator.layers):\n",
    "    print(f'layer {i}: {l}', flush=True)\n",
    "    print(f'has input mask: {l.input_mask}', flush=True)\n",
    "    print(f'has output mask: {l.output_mask}', flush=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
